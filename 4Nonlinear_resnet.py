import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import GradScaler, autocast
import torchvision
import torchvision.transforms as T
from tqdm import tqdm
import math

"""
### ğŸ’» æ··åˆéçº¿æ€§ResNetä»£ç è®¾è®¡æ€è·¯ (V2 - Bottleneck)

#### 1. æ ¸å¿ƒé—®é¢˜
V1ç‰ˆæœ¬çš„ `HybridNonLinearBlock` (conv -> nonlinear_conv) ä¾ç„¶ä¼šåœ¨é€šé“æ•°é«˜çš„å±‚ï¼ˆå¦‚ Layer3, 256x256ï¼‰å¯¼è‡´æ˜¾å­˜å¼€é”€å·¨å¤§ã€‚

#### 2. è§£å†³æ–¹æ¡ˆ (é‡‡çº³ç”¨æˆ·å»ºè®®)
æ ¹æ®ç”¨æˆ·çš„å»ºè®®ï¼Œæˆ‘ä»¬é‡‡ç”¨"å°†éçº¿æ€§å·ç§¯å¯¹åº”çš„é€šé“æ•°å‡å°"çš„ç­–ç•¥ï¼Œè®¾è®¡ä¸€ä¸ª"éçº¿æ€§ç“¶é¢ˆ"æ¨¡å— (HybridNonLinearBottleneck)ï¼Œå…¶ç»“æ„å¦‚ä¸‹ï¼š

1.  `conv1` (çº¿æ€§ `1x1`):  å°†é€šé“æ•° `in_planes` (å¦‚ 256) **é™ç»´**åˆ° `planes` (å¦‚ 64)ã€‚
2.  `conv2` (éçº¿æ€§ `3x3`): åœ¨**ä½ç»´** (64x64) ä¸Šæ‰§è¡Œæ˜‚è´µçš„ `NonLinearConvBlock`ã€‚
3.  `conv3` (çº¿æ€§ `1x1`):  å°†é€šé“æ•° `planes` (å¦‚ 64) **å‡ç»´**å› `planes * expansion` (å¦‚ 256)ã€‚

#### 3. ä¼˜åŠ¿
- æ˜¾å­˜å¼€é”€ï¼ˆä¸»è¦åœ¨conv2ï¼‰çš„ `C_in * C_out` å¤æ‚åº¦ä» `256*256` éª¤é™åˆ° `64*64`ï¼Œ**æå¤§ç¼“è§£æ˜¾å­˜**ã€‚
- å…è®¸æˆ‘ä»¬åœ¨ç½‘ç»œçš„æ¯ä¸€å±‚ï¼ˆ`layer2`, `layer3`, `layer4`ï¼‰éƒ½ä½¿ç”¨éçº¿æ€§è®¡ç®—ï¼Œè€Œä¸ä¼šOOMã€‚
- è¿™å®Œå…¨ç¬¦åˆç”¨æˆ·"å‡å°éçº¿æ€§å·ç§¯é€šé“æ•°"å’Œ"æ··åˆä½¿ç”¨"çš„æ€æƒ³ã€‚
"""

# ==============================================================================
# 1. æ ¸å¿ƒæ¨¡å—ï¼šéçº¿æ€§å·ç§¯ (EKV Model)
#    ã€æ³¨æ„ã€‘è¿™é‡Œå¿…é¡»ä½¿ç”¨ä¸Šä¸€ç‰ˆå›å¤ä¸­å¸¦ for å¾ªç¯çš„ forward æ–¹æ³•ï¼
# ==============================================================================

class NonLinearConv2d(nn.Module):
    """
    æ¨¡æ‹Ÿ EKV æ¨¡å‹çš„éçº¿æ€§å·ç§¯å±‚ã€‚
    è¾“å…¥ï¼šV_G (ç”µå‹), æƒé‡ï¼šV_th (é˜ˆå€¼ç”µå‹)
    è¾“å‡ºï¼šI_k (ç”µæµ)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size, kernel_size)
        self.kernel_size = kernel_size
        
        if isinstance(stride, int):
            stride = (stride, stride)
        self.stride = stride
        
        if isinstance(padding, int):
            padding = (padding, padding)
        self.padding = padding

        self.theta = nn.Parameter(
            torch.empty(out_channels, in_channels, *kernel_size)
        )
        nn.init.kaiming_uniform_(self.theta, a=math.sqrt(5))
        
        self.alpha = 0.0005625
        self.VD = 0.1
        self.n = 1.5
        self.VT = 0.025
        self.denom = 2 * self.n * self.VT 

        self.v_min = 0.0
        self.v_max = 9.0
        self.theta_min = 1.0
        self.theta_max = 8.0

    def _ekv_f(self, v_in, v_th):
        """ EKV æ ¸å¿ƒæ–¹ç¨‹ f(V, Î¸) """
        arg = (v_in - v_th) / self.denom
        arg = torch.clamp(arg, -50, 50) 
        return torch.pow(torch.log(1 + torch.exp(arg)), 2)

    def forward(self, x):
        # 1. ç‰©ç†çº¦æŸï¼šé’³ä½æƒé‡å’Œè¾“å…¥
        self.theta.data.clamp_(self.theta_min, self.theta_max)
        x_clamped = torch.clamp(x, self.v_min, self.v_max)

        # 2. å±•å¼€è¾“å…¥ä¸º Patches
        patches = F.unfold(
            x_clamped, 
            self.kernel_size, 
            stride=self.stride, 
            padding=self.padding
        )
        # patches shape: (B, C_in * K * K, L)
        
        B, Cin_K_K, L = patches.shape
        
        # 3. å‡†å¤‡å¹¿æ’­ (V_G)
        # (B, C_in*K*K, L) -> (B, L, C_in*K*K)
        v_g = patches.transpose(1, 2)

        # å‡†å¤‡æƒé‡ (V_th)
        # (C_out, C_in, K, K) -> (C_out, C_in*K*K)
        v_th_flat = self.theta.view(self.out_channels, -1)

        # 4. ã€è§£å†³æ–¹æ¡ˆã€‘è¿­ä»£ C_outï¼Œç”¨æ—¶é—´æ¢ç©ºé—´ (è§£å†³OOMçš„å…³é”®)
        i_k_list = []
        for i in range(self.out_channels):
            # v_g shape:         (B, L, C_in*K*K)
            # v_th_channel shape: (1, 1, C_in*K*K)
            v_th_channel = v_th_flat[i].unsqueeze(0).unsqueeze(0)
            
            term1 = self._ekv_f(v_g, v_th_channel)
            term2 = self._ekv_f(v_g, v_th_channel + self.VD)
            
            # current_patches shape: (B, L, C_in*K*K)
            current_patches = self.alpha * (term1 - term2)

            # 5. æ¨¡æ‹Ÿ KCLï¼šç”µæµæ±‚å’Œ
            # i_k_channel shape: (B, L)
            i_k_channel = current_patches.sum(dim=2)
            i_k_list.append(i_k_channel)

        # 6. æ‹¼æ¥æ‰€æœ‰è¾“å‡ºé€šé“
        # List[ (B, L) ] -> (B, C_out, L)
        i_k = torch.stack(i_k_list, dim=1)
        
        # 7. è½¬æ¢å›å›¾åƒæ ¼å¼
        out_h = (x.shape[2] + 2 * self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1
        out_w = (x.shape[3] + 2 * self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1
        
        out = i_k.reshape(B, self.out_channels, out_h, out_w)
        
        return out

# ==============================================================================
# 2. æ¨¡æ‹Ÿç”µè·¯å°è£… (Block) - (ä¿æŒä¸å˜)
# ==============================================================================

class NonLinearConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = NonLinearConv2d(
            in_channels, out_channels, kernel_size, stride, padding
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.v_min = 0.0
        self.v_max = 9.0

    def forward(self, x):
        i_k = self.conv(x)
        v_out = self.bn(i_k)
        v_clamped = torch.clamp(v_out, self.v_min, self.v_max)
        return v_clamped

# ==============================================================================
# 3. æ··åˆ ResNet æ¶æ„
# ==============================================================================

class BasicBlock(nn.Module):
    """æ ‡å‡† ResNet BasicBlock - (ä¿æŒä¸å˜)"""
    expansion = 1
    # ... (ä»£ç åŒå‰ï¼Œä¸ºç®€æ´çœç•¥) ...
    def __init__(self, in_planes, planes, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(
            planes, planes, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes, self.expansion * planes,
                    kernel_size=1, stride=stride, bias=False
                ),
                nn.BatchNorm2d(self.expansion * planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


# ã€æ–°æ¨¡å—ã€‘ å®ç°æ‚¨çš„æ–¹æ¡ˆäºŒ
class HybridNonLinearBottleneck(nn.Module):
    """
    éçº¿æ€§ç“¶é¢ˆæ¨¡å— (ResNet-50 é£æ ¼)
    conv1 (çº¿æ€§ 1x1, é™ç»´) -> conv2 (éçº¿æ€§ 3x3) -> conv3 (çº¿æ€§ 1x1, å‡ç»´)
    """
    expansion = 4 # å‡ç»´/é™ç»´å› å­

    def __init__(self, in_planes, planes, stride=1):
        super().__init__()
        
        # 1. çº¿æ€§ 1x1 é™ç»´
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu1 = nn.ReLU(inplace=True)

        # 2. éçº¿æ€§ 3x3 (åœ¨ä½ç»´ planes ä¸Šè®¡ç®—)
        self.conv2_nonlinear = NonLinearConvBlock(
            planes, planes, kernel_size=3, stride=stride, padding=1
        )
        # æ³¨æ„ï¼šNonLinearConvBlock å†…éƒ¨å·²ç»æœ‰ BN å’Œ Clampï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦

        # 3. çº¿æ€§ 1x1 å‡ç»´
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)

        # Shortcut
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes, self.expansion * planes,
                    kernel_size=1, stride=stride, bias=False
                ),
                nn.BatchNorm2d(self.expansion * planes)
            )

    def forward(self, x):
        identity = self.shortcut(x)

        out = self.relu1(self.bn1(self.conv1(x)))
        out = self.conv2_nonlinear(out)
        out = self.bn3(self.conv3(out))

        out += identity # ç”µå‹ç›¸åŠ 
        # åŒæ ·ï¼Œæœ«å°¾æ²¡æœ‰æ¿€æ´»å‡½æ•°
        return out


# ã€ä¿®æ”¹åçš„ä¸»ç½‘ç»œã€‘
class HybridResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_planes = 64

        # æ ‡å‡† conv1
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        # æ··åˆæ¶æ„
        # æˆ‘ä»¬å¯ä»¥é€‰æ‹©åœ¨å“ªä¸€å±‚ä½¿ç”¨éçº¿æ€§æ¨¡å—
        # è¿™é‡Œæ¼”ç¤ºï¼šlayer1ç”¨æ ‡å‡†ï¼Œlayer2,3,4ç”¨éçº¿æ€§ç“¶é¢ˆ
        self.layer1 = self._make_layer(BasicBlock, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        # æœ€ç»ˆçš„ in_planes æ˜¯ 256 * expansion
        self.linear = nn.Linear(256 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for s in strides:
            layers.append(block(self.in_planes, planes, s))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avgpool(out)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

def HybridResNet_Bottleneck():
    # ç±»ä¼¼ ResNet-50 çš„ [3, 4, 6, 3] ç»“æ„
    return HybridResNet(HybridNonLinearBottleneck, [3, 4, 6, 3])

# ==============================================================================
# 4. è®­ç»ƒå’Œè¯„ä¼° (ä¿æŒä¸å˜)
# ==============================================================================
# ... (train_one_epoch, evaluate å‡½æ•°åŒå‰ï¼Œä¸ºç®€æ´çœç•¥) ...
def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device, epoch, total_epochs, warmup_epochs, clip_norm):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    loader_tqdm = tqdm(loader, desc=f"Epoch {epoch+1}/{total_epochs} [Train]")
    
    for i, (inputs, labels) in enumerate(loader_tqdm):
        inputs, labels = inputs.to(device), labels.to(device)

        if epoch < warmup_epochs:
            lr = LEARNING_RATE * (epoch * len(loader) + i) / (warmup_epochs * len(loader))
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
        
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        optimizer.zero_grad()
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)
        scaler.step(optimizer)
        scaler.update()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        loader_tqdm.set_postfix(
            loss=running_loss/(i+1), 
            acc=100.*correct/total,
            lr=optimizer.param_groups[0]['lr']
        )
    
    if epoch >= warmup_epochs:
        scheduler.step()

    return running_loss / len(loader), 100. * correct / total

def evaluate(model, loader, criterion, device, epoch, total_epochs):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    loader_tqdm = tqdm(loader, desc=f"Epoch {epoch+1}/{total_epochs} [Test]")
    
    with torch.no_grad():
        for inputs, labels in loader_tqdm:
            inputs, labels = inputs.to(device), labels.to(device)
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            loader_tqdm.set_postfix(
                loss=running_loss/len(loader), 
                acc=100.*correct/total
            )

    return running_loss / len(loader), 100. * correct / total
# ==============================================================================
# 5. ä¸»ç¨‹åº (ä¿®æ”¹)
# ==============================================================================

if __name__ == "__main__":
    
    # --- è¶…å‚æ•° ---
    DEVICE = "cuda:1" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {DEVICE}")

    BATCH_SIZE = 64 # ã€å»ºè®®ã€‘é‰´äºæ¨¡å‹æ›´å¤æ‚ï¼Œå…ˆä» 64 å¼€å§‹å°è¯•
    LEARNING_RATE = 1e-4 
    EPOCHS = 100
    WARMUP_EPOCHS = 10
    CLIP_NORM = 1.0 
    
    # --- æ•°æ®åŠ è½½ (CIFAR-10) ---
    print("Preparing CIFAR-10 dataset...")
    transform_train = T.Compose([
        T.RandomCrop(32, padding=4),
        T.RandomHorizontalFlip(),
        T.ToTensor(),
        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 2.2010)),
    ])
    transform_test = T.Compose([
        T.ToTensor(),
        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    trainloader = torch.utils.data.DataLoader(
        trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True
    )
    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True
    )

    # --- æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨ ---
    print("Building HybridResNet_Bottleneck...")
    
    # ã€ä¿®æ”¹ã€‘è°ƒç”¨æ–°çš„ Bottleneck æ¨¡å‹
    model = HybridResNet_Bottleneck().to(DEVICE)
    # print(model) # å–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹æ¨¡å‹ç»“æ„
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)
    scaler = GradScaler()
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS - WARMUP_EPOCHS)

    # --- è®­ç»ƒå¾ªç¯ ---
    print("Starting training...")
    for epoch in range(EPOCHS):
        train_loss, train_acc = train_one_epoch(
            model, trainloader, criterion, optimizer, scaler, scheduler, DEVICE, 
            epoch, EPOCHS, WARMUP_EPOCHS, CLIP_NORM
        )
        test_loss, test_acc = evaluate(
            model, testloader, criterion, DEVICE, epoch, EPOCHS
        )
        
        print(f"Epoch {epoch+1}/{EPOCHS} Summary:")
        print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Test Loss:  {test_loss:.4f}, Test Acc:  {test_acc:.2f}%")
        print("-" * 30)

    print("Training finished.")